#!/usr/bin/env python3
"""
Test script for PromptWriter Agent integration.
"""

import sys
import os

# Add tools directory to path
sys.path.insert(0, '.claude/tools')

from github_issue import create_issue

def test_prompt_writer_workflow():
    """Test the full PromptWriter workflow."""

    print("Testing PromptWriter Agent Integration")
    print("=" * 50)

    # Example prompt that would be generated by PromptWriter
    test_prompt = """# Feature: User Dashboard Analytics

## Objective
Implement a comprehensive analytics dashboard for users to track their activity and performance metrics.

## Context
Users have requested better visibility into their usage patterns and performance metrics. This feature will provide a dedicated dashboard with charts, statistics, and insights.

## Requirements
- [ ] Activity timeline showing recent actions
- [ ] Performance metrics with trend analysis
- [ ] Customizable date range filters
- [ ] Export functionality for data (CSV/PDF)
- [ ] Mobile-responsive design

## Technical Considerations
- Use existing data aggregation service
- Implement client-side caching for performance
- Ensure GDPR compliance for data display

## Success Criteria
- [ ] Dashboard loads in under 2 seconds
- [ ] All metrics accurately reflect user data
- [ ] Exports contain complete data sets
- [ ] Mobile experience matches desktop functionality

## Dependencies
- Data aggregation service API
- Charting library (recommend Chart.js)
- PDF generation service

## Estimated Complexity: Medium
**Rationale**: Requires integration with multiple services and responsive design considerations."""

    print("\n1. Generated Prompt (from PromptWriter):")
    print("-" * 40)
    print(test_prompt[:500] + "..." if len(test_prompt) > 500 else test_prompt)

    print("\n2. Prompt Quality Validation:")
    print("-" * 40)

    # Simulate quality checks that PromptWriter would do
    quality_checks = [
        ("Has objective", "## Objective" in test_prompt),
        ("Has requirements", "## Requirements" in test_prompt),
        ("Has success criteria", "## Success Criteria" in test_prompt),
        ("Has complexity assessment", "## Estimated Complexity" in test_prompt),
        ("Has actionable items", "- [ ]" in test_prompt),
    ]

    score = 0
    for check_name, passed in quality_checks:
        status = "✓" if passed else "✗"
        print(f"  {status} {check_name}")
        if passed:
            score += 20

    print(f"\n  Quality Score: {score}/100")

    if score >= 80:
        print("  Status: PASSED - Ready for issue creation")
    else:
        print("  Status: FAILED - Needs improvement")
        return

    print("\n3. GitHub Issue Creation (dry run):")
    print("-" * 40)

    # Extract title from the prompt
    title = "Feature: User Dashboard Analytics"

    print(f"  Title: {title}")
    print(f"  Labels: ['enhancement', 'feature']")
    print(f"  Body length: {len(test_prompt)} characters")

    print("\n4. Integration Points:")
    print("-" * 40)
    print("  ✓ PromptWriter generates structured prompt")
    print("  ✓ Quality validation passes (80%+ score)")
    print("  ✓ GitHub issue tool ready for creation")
    print("  ✓ Optional architect review available")

    print("\n5. Would create issue with command:")
    print("-" * 40)
    print("  gh issue create \\")
    print(f"    --title '{title}' \\")
    print("    --body '[prompt content]' \\")
    print("    --label 'enhancement,feature'")

    print("\n" + "=" * 50)
    print("Integration test completed successfully!")
    print("\nTo create a real issue, run with --create flag")

    # Check if user wants to create real issue
    if len(sys.argv) > 1 and sys.argv[1] == '--create':
        print("\nCreating real GitHub issue...")
        result = create_issue(
            title=title,
            body=test_prompt,
            labels=["enhancement", "feature"]
        )

        if result['success']:
            print(f"✓ Created issue #{result['issue_number']}")
            print(f"  View at: {result['issue_url']}")
        else:
            print(f"✗ Failed: {result['error']}")

if __name__ == "__main__":
    test_prompt_writer_workflow()