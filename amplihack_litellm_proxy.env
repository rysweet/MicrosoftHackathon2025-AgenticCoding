# LiteLLM Standalone Proxy Configuration for amplihack
# This configuration tells amplihack to use a standalone LiteLLM proxy instead of the integrated one

# Point Claude Code to our standalone LiteLLM proxy server
OPENAI_API_KEY=proxy-key  # pragma: allowlist secret

# Use the original Azure Responses API endpoint - let amplihack handle the proxy
OPENAI_BASE_URL="https://ai-adapt-oai-eastus2.openai.azure.com/openai/v1/responses?api-version=preview"

# Enable the integrated proxy to use our LiteLLM configuration
AMPLIHACK_USE_LITELLM=true

# Basic proxy settings
PROXY_TYPE=litellm_standalone
PROXY_MODE=external
HOST=127.0.0.1
PORT=9001

# Performance settings
MAX_TOKENS_LIMIT=512000
MIN_TOKENS_LIMIT=4096
REQUEST_TIMEOUT=300
MAX_RETRIES=2
LOG_LEVEL=INFO

# Model mappings for compatibility
BIG_MODEL=gpt-5
MIDDLE_MODEL=gpt-5
SMALL_MODEL=gpt-5

# Azure settings for Responses API integration
AZURE_OPENAI_KEY=b892dc164a634fc49bd07bcbbf9d1a76  # pragma: allowlist secret
AZURE_OPENAI_API_KEY=b892dc164a634fc49bd07bcbbf9d1a76  # pragma: allowlist secret
AZURE_OPENAI_API_VERSION=2024-12-01
AZURE_OPENAI_ENDPOINT=https://ai-adapt-oai-eastus2.openai.azure.com
AZURE_OPENAI_BASE_URL="https://ai-adapt-oai-eastus2.openai.azure.com/openai/v1/responses?api-version=preview"
