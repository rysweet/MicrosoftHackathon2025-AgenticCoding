# Example Configuration: Azure Responses API Endpoint
# This configuration uses the Azure Responses API (/responses)
# Detection: URL contains "/responses" → Routes through direct Azure calls → Azure Responses API

# Required: Your Azure OpenAI API key
OPENAI_API_KEY="your-azure-api-key-here"  # pragma: allowlist secret

# Required: Azure Responses API endpoint URL - the proxy detects "/responses" in the URL
OPENAI_BASE_URL="https://your-resource.cognitiveservices.azure.com/openai/responses?api-version=2025-04-01-preview"

# Required: Azure-specific settings
AZURE_OPENAI_KEY="your-azure-api-key-here"  # pragma: allowlist secret
AZURE_API_VERSION="2025-04-01-preview"

# Model mappings - use gpt-5-codex for Responses API
BIG_MODEL="gpt-5-codex"
MIDDLE_MODEL="gpt-5-codex"
SMALL_MODEL="gpt-5-codex"

# Proxy settings
AMPLIHACK_USE_LITELLM=false  # Use integrated proxy routing
PROXY_TYPE=integrated_azure_responses
HOST=127.0.0.1
PORT=8000

# Performance settings
MAX_TOKENS_LIMIT=512000
REQUEST_TIMEOUT=300
MAX_RETRIES=2
LOG_LEVEL=INFO

# Note: Responses API is optimized for Claude models and tool calling
# It provides enhanced streaming and better tool execution compared to Chat API

# Example endpoints (replace with your actual Azure resource):
# https://ai-adapt-oai-eastus2.cognitiveservices.azure.com/openai/responses?api-version=2025-04-01-preview
