# Responses API Proxy Configuration - Routes gpt-5-codex to Responses endpoint
# For testing: uvx amplihack launch --with-proxy-config proxy_config_responses_api.env

OPENAI_API_KEY=proxy-key  # pragma: allowlist secret
OPENAI_BASE_URL="https://your-resource.cognitiveservices.azure.com/openai/responses"

# Enable LiteLLM routing
AMPLIHACK_USE_LITELLM=true
USE_LITELLM_ROUTER=true

# Proxy settings
PROXY_TYPE=integrated
PROXY_MODE=internal
HOST=127.0.0.1
PORT=8000

# Azure Responses API settings - REPLACE WITH YOUR VALUES
# Note: Use AZURE_OPENAI_API_KEY (not AZURE_OPENAI_KEY)
AZURE_OPENAI_API_KEY=YOUR_API_KEY_HERE  # pragma: allowlist secret
AZURE_OPENAI_API_VERSION=2025-04-01-preview
AZURE_OPENAI_ENDPOINT=https://your-resource.cognitiveservices.azure.com

# Model configuration - Responses API uses gpt-5-codex
BIG_MODEL=gpt-5-codex
MIDDLE_MODEL=gpt-5-codex
SMALL_MODEL=gpt-5-codex

# Performance settings
MAX_TOKENS_LIMIT=512000
REQUEST_TIMEOUT=300
