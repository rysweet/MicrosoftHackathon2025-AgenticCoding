{
  "review_metadata": {
    "review_type": "WAVE 1: Memory/Neo4j System Review",
    "focus": "Database correctness, connection management, query safety, transaction handling, schema design, performance",
    "timestamp": "2025-11-08",
    "reviewer": "Database Agent",
    "files_reviewed": [
      "src/amplihack/memory/database.py",
      "src/amplihack/memory/manager.py",
      "src/amplihack/memory/models.py",
      "src/amplihack/memory/maintenance.py",
      "src/amplihack/memory/neo4j/connector.py",
      "src/amplihack/memory/neo4j/config.py",
      "src/amplihack/memory/neo4j/memory_store.py",
      "src/amplihack/memory/neo4j/lifecycle.py",
      "src/amplihack/memory/neo4j/port_manager.py",
      "src/amplihack/memory/neo4j/exceptions.py",
      "src/amplihack/memory/neo4j/auto_setup.py",
      "src/amplihack/memory/neo4j/startup_wizard.py"
    ]
  },
  "critical_issues": [
    {
      "id": "CRIT-001",
      "severity": "critical",
      "category": "connection_leak",
      "file": "src/amplihack/memory/database.py",
      "line_range": "45-64",
      "title": "Connection Leak: No Connection Pooling or Resource Management",
      "description": "MemoryDatabase creates new connections on every operation via _get_connection(). Each call creates a fresh sqlite3.Connection without pooling. If operations are concurrent, this leaks connections and can exhaust file descriptors.",
      "code_snippet": "def _get_connection(self) -> sqlite3.Connection:\n    conn = sqlite3.connect(self.db_path, timeout=30.0, check_same_thread=False)\n    # No connection pooling, no reuse, no tracking",
      "impact": "Connection exhaustion, file descriptor leaks, degraded performance under concurrent access",
      "fix": "Implement connection pooling or store a single connection with proper lifecycle management. Consider using a connection pool or context manager that properly closes connections.",
      "references": ["https://docs.python.org/3/library/sqlite3.html#sqlite3-connection-context-manager"]
    },
    {
      "id": "CRIT-002",
      "severity": "critical",
      "category": "data_integrity",
      "file": "src/amplihack/memory/database.py",
      "line_range": "136-182",
      "title": "No Explicit Transaction Control in store_memory",
      "description": "store_memory() calls conn.commit() at line 177 but does not use explicit BEGIN/COMMIT or handle rollback on errors. The try-except catches sqlite3.Error but only prints and returns False, leaving database in undefined state if partial writes occur.",
      "code_snippet": "try:\n    with self._get_connection() as conn:\n        self._update_session(conn, memory.session_id, memory.agent_id)\n        conn.execute(\"INSERT OR REPLACE...\")  # Two operations without transaction\n        conn.commit()  # What if _update_session fails?",
      "impact": "Partial writes, data inconsistency, violated invariants (session exists but memory doesn't)",
      "fix": "Use explicit BEGIN/COMMIT with proper rollback. Ensure atomic operations: either both _update_session AND memory insert succeed, or neither does.",
      "references": ["https://www.sqlite.org/atomiccommit.html"]
    },
    {
      "id": "CRIT-003",
      "severity": "critical",
      "category": "injection_risk",
      "file": "src/amplihack/memory/database.py",
      "line_range": "196-239",
      "title": "SQL Injection Risk in retrieve_memories via Dynamic Query Building",
      "description": "retrieve_memories() builds SQL dynamically using string formatting (line 198: f'SELECT...WHERE {where_clause}'). If query.to_sql_where() doesn't properly escape all inputs, injection is possible. Lines 207-210 directly inject query.limit/offset into SQL string without parameterization.",
      "code_snippet": "sql = f\"SELECT ... WHERE {where_clause}\"  # where_clause from user input\nif query.limit:\n    sql += f\" LIMIT {query.limit}\"  # Direct injection\nif query.offset:\n    sql += f\" OFFSET {query.offset}\"  # Direct injection",
      "impact": "SQL injection vulnerability if limit/offset are not validated, potential data exfiltration or corruption",
      "fix": "Use parameterized LIMIT/OFFSET. Verify query.to_sql_where() properly escapes all inputs. Validate limit/offset are integers.",
      "references": ["https://owasp.org/www-community/attacks/SQL_Injection"]
    },
    {
      "id": "CRIT-004",
      "severity": "critical",
      "category": "performance",
      "file": "src/amplihack/memory/database.py",
      "line_range": "221-233",
      "title": "N+1 Update Pattern After Retrieval",
      "description": "retrieve_memories() fetches memories in one query, then issues separate UPDATE for each memory (lines 225-232). For N memories, this is 1 SELECT + N UPDATEs = N+1 queries. Under high concurrency or large result sets, this is extremely inefficient.",
      "code_snippet": "memories = []\nfor row in rows:\n    memory = self._row_to_memory(row)\n    memories.append(memory)\n# Later: separate UPDATE for each memory ID\nconn.execute(f\"UPDATE memory_entries SET accessed_at = ? WHERE id IN ({placeholders})\")",
      "impact": "Severe performance degradation with large result sets, locks database during N updates",
      "fix": "Batch UPDATE into single query with WHERE id IN (...). Already partially done but logic is confused. Simplify to single UPDATE with all IDs.",
      "references": []
    },
    {
      "id": "CRIT-005",
      "severity": "critical",
      "category": "connection_management",
      "file": "src/amplihack/memory/neo4j/connector.py",
      "line_range": "220-246",
      "title": "No Connection Pooling Cleanup on Error",
      "description": "Neo4jConnector.connect() creates GraphDatabase.driver() at line 233 but doesn't close it on connection failure. If ServiceUnavailable is raised, driver remains open but unusable. No cleanup in __del__ or finalizer.",
      "code_snippet": "try:\n    self._driver = GraphDatabase.driver(self.uri, auth=(self.user, self.password))\n    return self\nexcept ServiceUnavailable as e:\n    logger.error(...)\n    raise  # Driver leaked!",
      "impact": "Connection pool exhaustion, resource leaks, orphaned driver objects",
      "fix": "Ensure driver.close() is called on exception. Add try-except-finally or cleanup on error.",
      "references": ["https://neo4j.com/docs/api/python-driver/current/api.html#driver"]
    },
    {
      "id": "CRIT-006",
      "severity": "critical",
      "category": "transaction_safety",
      "file": "src/amplihack/memory/neo4j/memory_store.py",
      "line_range": "333-379",
      "title": "Partial Transaction in record_usage with Multiple Updates",
      "description": "record_usage() performs 4 separate operations in single transaction (lines 334-363): CREATE USED relationship, UPDATE application_count, recalculate success_rate, UPDATE quality_score. If any fails, entire transaction should rollback, but error handling (lines 374-379) only logs and returns False without verifying rollback occurred.",
      "code_snippet": "CREATE (ai)-[:USED]->(m)\nSET m.application_count = m.application_count + 1\nMATCH (m)<-[u:USED]-()\nSET m.success_rate = toFloat(successes) / toFloat(total_uses)\nSET m.quality_score = (m.quality_score * 0.9 + $feedback_score * 0.1)",
      "impact": "Inconsistent state if transaction partially succeeds. Success_rate or quality_score could be wrong if intermediate steps fail.",
      "fix": "Ensure atomic transaction. Verify Neo4j driver properly handles rollback on exception. Add explicit transaction verification.",
      "references": ["https://neo4j.com/docs/cypher-manual/current/clauses/call-subquery/#subquery-transactions"]
    },
    {
      "id": "CRIT-007",
      "severity": "critical",
      "category": "cypher_injection",
      "file": "src/amplihack/memory/neo4j/memory_store.py",
      "line_range": "220-225",
      "title": "Potential Cypher Injection in Dynamic SET Clause",
      "description": "update_memory() builds SET clause dynamically using string formatting (line 222: f'SET {', '.join(updates)}'). While parameters are used for values, the SET clause itself is constructed from updates list which could be manipulated if updates list construction is compromised.",
      "code_snippet": "updates = []\nif content is not None:\n    updates.append('m.content = $content')\n# ...\nquery = f\"MATCH (m:Memory {{id: $memory_id}}) SET {', '.join(updates)}\"",
      "impact": "Potential Cypher injection if updates list is compromised. Low probability but high impact.",
      "fix": "Use whitelist validation for property names. Ensure updates list only contains validated, hardcoded property names.",
      "references": ["https://neo4j.com/developer/kb/protecting-against-cypher-injection/"]
    },
    {
      "id": "CRIT-008",
      "severity": "critical",
      "category": "data_loss",
      "file": "src/amplihack/memory/maintenance.py",
      "line_range": "40-99",
      "title": "No Confirmation or Backup Before Bulk Deletion",
      "description": "cleanup_old_sessions() deletes sessions and memories without backup or confirmation (lines 71-81). If called accidentally with wrong cutoff_date, data is irrecoverably lost. No transaction rollback capability.",
      "code_snippet": "conn.execute('DELETE FROM memory_entries WHERE session_id = ?', (session.session_id,))\nconn.execute('DELETE FROM session_agents WHERE session_id = ?', (session.session_id,))\nconn.execute('DELETE FROM sessions WHERE session_id = ?', (session.session_id,))",
      "impact": "Permanent data loss if called with wrong parameters. No undo mechanism.",
      "fix": "Add dry-run mode. Require explicit confirmation. Create backup before deletion. Add restore capability.",
      "references": []
    }
  ],
  "high_issues": [
    {
      "id": "HIGH-001",
      "severity": "high",
      "category": "missing_indexes",
      "file": "src/amplihack/memory/database.py",
      "line_range": "112-134",
      "title": "Missing Composite Index for Common Query Pattern",
      "description": "retrieve_memories often queries by (session_id, agent_id, memory_type) but only has idx_memory_session_agent covering (session_id, agent_id). Adding memory_type would require full table scan. No covering index for common access patterns.",
      "code_snippet": "# Existing:\nCREATE INDEX idx_memory_session_agent ON memory_entries(session_id, agent_id)\n# Missing:\nCREATE INDEX idx_memory_session_agent_type ON memory_entries(session_id, agent_id, memory_type)",
      "impact": "Slower queries when filtering by memory_type, full table scans on large datasets",
      "fix": "Add composite index: idx_memory_session_agent_type(session_id, agent_id, memory_type) and idx_memory_type_importance(memory_type, importance DESC) for common filters.",
      "references": []
    },
    {
      "id": "HIGH-002",
      "severity": "high",
      "category": "error_handling",
      "file": "src/amplihack/memory/database.py",
      "line_range": "180-182",
      "title": "Silent Error Handling with Print Instead of Logging",
      "description": "store_memory() catches sqlite3.Error and uses print() instead of proper logging (line 181). Errors are lost in production environments without stderr capture. Same pattern in multiple methods (retrieve_memories line 238, get_memory_by_id line 281, etc.).",
      "code_snippet": "except sqlite3.Error as e:\n    print(f'Database error storing memory {memory.id}: {e}')\n    return False",
      "impact": "Lost error information in production, difficult debugging, no error monitoring/alerting",
      "fix": "Replace all print() with logger.error(). Ensure logger is configured. Add structured logging for database errors.",
      "references": []
    },
    {
      "id": "HIGH-003",
      "severity": "high",
      "category": "race_condition",
      "file": "src/amplihack/memory/database.py",
      "line_range": "496-516",
      "title": "TOCTOU Race Condition in _update_session",
      "description": "_update_session uses COALESCE with SELECT subquery (lines 503-506) which has TOCTOU issue. Between checking if session exists and inserting, another thread could insert, causing unique constraint violation or data corruption.",
      "code_snippet": "INSERT OR REPLACE INTO sessions (session_id, created_at, last_accessed, metadata)\nVALUES (?, COALESCE((SELECT created_at FROM sessions WHERE session_id = ?), ?), ?, '{}')",
      "impact": "Race condition in concurrent writes, potential unique constraint violations, data corruption",
      "fix": "Use INSERT OR IGNORE followed by UPDATE, or use UPSERT with ON CONFLICT clause for atomic operation.",
      "references": ["https://www.sqlite.org/lang_UPSERT.html"]
    },
    {
      "id": "HIGH-004",
      "severity": "high",
      "category": "performance",
      "file": "src/amplihack/memory/database.py",
      "line_range": "387-448",
      "title": "N+1 Query Pattern in list_sessions",
      "description": "list_sessions() fetches all sessions in one query, then for EACH session makes 2 additional queries: one for agent_ids (lines 414-422) and one for memory_count (lines 425-431). For N sessions, this is 1 + 2N queries.",
      "code_snippet": "for row in cursor.fetchall():\n    session_id = row[0]\n    # Query 1: Get agent IDs\n    agent_cursor = conn.execute('SELECT agent_id FROM session_agents WHERE session_id = ?', (session_id,))\n    # Query 2: Get memory count\n    count_cursor = conn.execute('SELECT COUNT(*) FROM memory_entries WHERE session_id = ?', (session_id,))",
      "impact": "Severe performance degradation with many sessions, O(N) queries instead of O(1)",
      "fix": "Use JOINs or CTEs to fetch all data in single query. Example: LEFT JOIN session_agents, LEFT JOIN (SELECT session_id, COUNT(*) FROM memory_entries GROUP BY session_id).",
      "references": []
    },
    {
      "id": "HIGH-005",
      "severity": "high",
      "category": "type_safety",
      "file": "src/amplihack/memory/models.py",
      "line_range": "134-181",
      "title": "SQL Injection Risk in Tag Search Pattern",
      "description": "MemoryQuery.to_sql_where() builds tag search using LIKE with wildcards (lines 172-178). Tag values are wrapped in quotes but not properly escaped. A tag containing SQL metacharacters could break query.",
      "code_snippet": "for tag in self.tags:\n    tag_conditions.append('tags LIKE ?')\n    params.append(f'%\"{tag}\"%')  # Tag not escaped!",
      "impact": "SQL injection if tags contain quotes or SQL metacharacters, query syntax errors",
      "fix": "Properly escape tag values or use JSON functions for tag array queries. Validate tags don't contain SQL metacharacters.",
      "references": []
    },
    {
      "id": "HIGH-006",
      "severity": "high",
      "category": "connection_management",
      "file": "src/amplihack/memory/neo4j/connector.py",
      "line_range": "289-318",
      "title": "Exponential Backoff Without Jitter Causes Thundering Herd",
      "description": "Retry logic uses exponential backoff (line 301: wait_time = 2**attempt) but without jitter. If multiple clients fail simultaneously, they all retry at same intervals, causing thundering herd problem.",
      "code_snippet": "wait_time = 2**attempt  # All clients wait same time\ntime.sleep(wait_time)  # No jitter, synchronized retries",
      "impact": "Thundering herd on retry, increased server load, cascading failures",
      "fix": "Add jitter: wait_time = (2**attempt) * (1 + random.uniform(0, 0.1)). Randomize retry intervals.",
      "references": ["https://aws.amazon.com/blogs/architecture/exponential-backoff-and-jitter/"]
    },
    {
      "id": "HIGH-007",
      "severity": "high",
      "category": "transaction_correctness",
      "file": "src/amplihack/memory/neo4j/connector.py",
      "line_range": "364-372",
      "title": "Result Consumed Inside Transaction but Return Type Unclear",
      "description": "execute_write uses execute_write transaction function (line 372) and consumes result inside transaction (line 370: 'return [dict(record) for record in result]'). This is correct for avoiding lazy evaluation issues, but function doesn't document transaction semantics or retry behavior.",
      "code_snippet": "def _execute_tx(tx):\n    result = tx.run(query, parameters or {})\n    return [dict(record) for record in result]  # Good: consumed in tx\nreturn session.execute_write(_execute_tx)  # But: retry behavior unclear",
      "impact": "Unclear retry semantics, potential data duplication if query not idempotent and retried",
      "fix": "Document transaction retry behavior. Ensure queries are idempotent or track transaction IDs.",
      "references": ["https://neo4j.com/docs/api/python-driver/current/api.html#neo4j.Session.execute_write"]
    },
    {
      "id": "HIGH-008",
      "severity": "high",
      "category": "missing_indexes",
      "file": "src/amplihack/memory/neo4j/memory_store.py",
      "line_range": "277-313",
      "title": "No Index on Quality Score for Sorting",
      "description": "get_memories_by_agent_type sorts by quality_score DESC (line 308) but doesn't verify index exists. If Memory(quality_score) index missing, this causes full node scan on every query.",
      "code_snippet": "MATCH (at:AgentType {id: $agent_type})-[:HAS_MEMORY]->(m:Memory)\nWHERE m.quality_score >= $min_quality\nORDER BY m.quality_score DESC  # Needs index!",
      "impact": "Slow queries on large memory datasets, full node scans instead of index seek",
      "fix": "Create index: CREATE INDEX memory_quality_score IF NOT EXISTS FOR (m:Memory) ON (m.quality_score). Document index requirements.",
      "references": []
    },
    {
      "id": "HIGH-009",
      "severity": "high",
      "category": "data_integrity",
      "file": "src/amplihack/memory/neo4j/memory_store.py",
      "line_range": "72-147",
      "title": "No Verification That AgentType Exists Before Creating Memory",
      "description": "create_memory() uses MATCH (at:AgentType {id: $agent_type}) at line 74 but doesn't verify match succeeded. If agent_type doesn't exist, CREATE (m:Memory) at line 77 still executes, creating orphaned memory node.",
      "code_snippet": "MATCH (at:AgentType {id: $agent_type})  # What if this fails?\nCREATE (m:Memory { ... })  # Creates anyway!\nCREATE (at)-[:HAS_MEMORY]->(m)  # This will fail silently",
      "impact": "Orphaned memory nodes, referential integrity violations, data inconsistency",
      "fix": "Add WHERE clause or use MERGE for AgentType. Verify at exists before creating memory. Raise ValueError if not found.",
      "references": []
    },
    {
      "id": "HIGH-010",
      "severity": "high",
      "category": "performance",
      "file": "src/amplihack/memory/neo4j/memory_store.py",
      "line_range": "333-379",
      "title": "Inefficient Success Rate Calculation in record_usage",
      "description": "record_usage recalculates success_rate by scanning ALL USED relationships every time (lines 352-356). For memory with 1000 uses, this reads 1000 relationships on every update. O(N) cost per update.",
      "code_snippet": "MATCH (m)<-[u:USED]-()\nWITH m, count(u) as total_uses, size([x IN collect(u) WHERE x.outcome = 'successful']) as successes\nSET m.success_rate = toFloat(successes) / toFloat(total_uses)",
      "impact": "Performance degradation as usage count grows, O(N) per update becomes expensive",
      "fix": "Store success_count and total_count as properties. Update incrementally: SET m.success_count = m.success_count + 1, m.success_rate = m.success_count / m.total_count.",
      "references": []
    },
    {
      "id": "HIGH-011",
      "severity": "high",
      "category": "error_handling",
      "file": "src/amplihack/memory/neo4j/lifecycle.py",
      "line_range": "236-299",
      "title": "No Cleanup on Container Creation Failure",
      "description": "_create_container() runs docker run but doesn't cleanup on failure (lines 276-299). If container created but fails to start, orphaned container remains. No docker rm on error path.",
      "code_snippet": "result = subprocess.run([\"docker\", \"run\", \"-d\", \"--name\", self.config.container_name, ...])\nif result.returncode != 0:\n    logger.error(...)\n    return False  # Container leaked!",
      "impact": "Orphaned containers accumulate on repeated failures, name conflicts prevent recovery",
      "fix": "On failure, call docker rm <container_name> to cleanup. Check if container exists before creating.",
      "references": []
    },
    {
      "id": "HIGH-012",
      "severity": "high",
      "category": "security",
      "file": "src/amplihack/memory/neo4j/lifecycle.py",
      "line_range": "243-272",
      "title": "Passwords Passed as Command Line Arguments (Visible in ps/logs)",
      "description": "Container creation passes password via -e flag (line 258: f'NEO4J_AUTH=neo4j/{self.config.password}'). Passwords in command line are visible via ps, docker inspect, and system logs.",
      "code_snippet": "\"-e\", f\"NEO4J_AUTH=neo4j/{self.config.password}\",  # Password in plaintext!",
      "impact": "Password exposure via process list, docker logs, audit logs. Security vulnerability.",
      "fix": "Use docker secrets or environment file. Pass password via stdin or --env-file instead of -e flag.",
      "references": ["https://docs.docker.com/engine/swarm/secrets/"]
    }
  ],
  "medium_issues": [
    {
      "id": "MED-001",
      "severity": "medium",
      "category": "code_reuse",
      "file": "src/amplihack/memory/database.py + maintenance.py",
      "line_range": "multiple",
      "title": "Duplicate Query Logic in Multiple Methods",
      "description": "Session info retrieval logic is duplicated in get_session_info() (lines 328-385) and list_sessions() (lines 387-448). Both methods construct SessionInfo with same queries for agent_ids and memory_count.",
      "code_snippet": "# In get_session_info:\ncursor = conn.execute('SELECT agent_id FROM session_agents WHERE session_id = ?'...)\n# In list_sessions:\nagent_cursor = conn.execute('SELECT agent_id FROM session_agents WHERE session_id = ?'...)",
      "impact": "Code duplication, maintenance burden, inconsistency risk",
      "fix": "Extract common logic into _build_session_info(conn, session_id, session_row) helper method.",
      "references": []
    },
    {
      "id": "MED-002",
      "severity": "medium",
      "category": "error_handling",
      "file": "src/amplihack/memory/models.py",
      "line_range": "518-538",
      "title": "Broad Exception Catching Hides Specific Errors",
      "description": "_row_to_memory() catches broad (ValueError, TypeError, json.JSONDecodeError) but treats all same way (line 536-538). Different errors need different handling (ValueError might indicate corruption, json.JSONDecodeError is recoverable).",
      "code_snippet": "except (ValueError, TypeError, json.JSONDecodeError) as e:\n    print(f'Error converting row to memory: {e}')\n    return None",
      "impact": "Lost error context, difficult debugging, can't distinguish corruption from transient errors",
      "fix": "Handle each exception type separately. Log different severities. Consider raising for ValueError (data corruption).",
      "references": []
    },
    {
      "id": "MED-003",
      "severity": "medium",
      "category": "missing_validation",
      "file": "src/amplihack/memory/manager.py",
      "line_range": "31-97",
      "title": "No Validation of Input Parameters in store()",
      "description": "store() accepts title, content, etc but doesn't validate length or content. Very long title could exceed database limits. No check for empty content. Importance can be any int, not validated to 1-10 range.",
      "code_snippet": "def store(self, agent_id: str, title: str, content: str, ...):\n    # No validation of title length, content, or importance range",
      "impact": "Database errors on overly long input, data quality issues, unexpected behavior",
      "fix": "Add validation: title max 200 chars, content max 100KB, importance must be 1-10 if provided. Raise ValueError on invalid input.",
      "references": []
    },
    {
      "id": "MED-004",
      "severity": "medium",
      "category": "type_safety",
      "file": "src/amplihack/memory/manager.py",
      "line_range": "99-144",
      "title": "String Memory Type Not Validated Against Enum",
      "description": "retrieve() accepts memory_type as Union[MemoryType, str] and tries to convert string to enum (lines 126-130), but if conversion fails, raises ValueError. Caller can't distinguish invalid type from other errors.",
      "code_snippet": "if isinstance(memory_type, str):\n    try:\n        memory_type = MemoryType(memory_type.lower())\n    except ValueError:\n        raise ValueError(f'Invalid memory type: {memory_type}')  # Generic error",
      "impact": "Poor error messages, difficult debugging, no distinction between validation and other errors",
      "fix": "Create custom MemoryTypeError exception. Provide list of valid types in error message.",
      "references": []
    },
    {
      "id": "MED-005",
      "severity": "medium",
      "category": "code_reuse",
      "file": "src/amplihack/memory/neo4j/connector.py",
      "line_range": "274-318 and 346-394",
      "title": "Duplicate Retry Logic in execute_query and execute_write",
      "description": "_execute_query_internal() and _execute_write_internal() have identical retry logic (exponential backoff, max_retries, error handling). Only difference is session.run vs session.execute_write.",
      "code_snippet": "# In both methods:\nfor attempt in range(self.max_retries):\n    try:\n        # ...\n    except ServiceUnavailable as e:\n        wait_time = 2**attempt\n        time.sleep(wait_time)",
      "impact": "Code duplication, maintenance burden, inconsistency risk if one is updated but not other",
      "fix": "Extract common retry logic into _execute_with_retry(func, query, params) helper method that wraps both read and write operations.",
      "references": []
    },
    {
      "id": "MED-006",
      "severity": "medium",
      "category": "missing_constraint",
      "file": "src/amplihack/memory/neo4j/memory_store.py",
      "line_range": "68-147",
      "title": "No Constraint Preventing Duplicate Memory IDs",
      "description": "create_memory() generates UUID for memory_id (line 69) but doesn't verify uniqueness. No UNIQUE constraint on Memory(id). Collision probability is astronomically low but not zero. No graceful handling if collision occurs.",
      "code_snippet": "memory_id = str(uuid4())  # No uniqueness check\nCREATE (m:Memory { id: $memory_id, ... })  # No constraint enforcing uniqueness",
      "impact": "Extremely low probability data corruption if UUID collision occurs, no detection mechanism",
      "fix": "Create UNIQUE constraint: CREATE CONSTRAINT memory_id_unique IF NOT EXISTS FOR (m:Memory) REQUIRE m.id IS UNIQUE. Handle constraint violation gracefully.",
      "references": []
    },
    {
      "id": "MED-007",
      "severity": "medium",
      "category": "performance",
      "file": "src/amplihack/memory/neo4j/memory_store.py",
      "line_range": "444-495",
      "title": "Content Search Uses CONTAINS Without Full-Text Index",
      "description": "search_memories() uses CONTAINS for content search (line 464: 'm.content CONTAINS $query'). Without full-text index, this causes full node scan. For large memory graphs, this is very slow.",
      "code_snippet": "WHERE m.content CONTAINS $query OR any(tag IN m.tags WHERE tag CONTAINS $query)",
      "impact": "Slow searches on large datasets, full node scans, poor user experience",
      "fix": "Create full-text index: CREATE FULLTEXT INDEX memory_content_fulltext IF NOT EXISTS FOR (m:Memory) ON EACH [m.content, m.tags]. Use db.index.fulltext.queryNodes().",
      "references": ["https://neo4j.com/docs/cypher-manual/current/indexes-for-full-text-search/"]
    },
    {
      "id": "MED-008",
      "severity": "medium",
      "category": "type_safety",
      "file": "src/amplihack/memory/neo4j/config.py",
      "line_range": "73-79",
      "title": "Port Validation Allows Privileged Ports",
      "description": "from_environment() validates ports are 1024-65535 (lines 74-77) but allows privileged ports (1024-49151 includes many system services). No check for common service port conflicts (PostgreSQL 5432, MySQL 3306, etc.).",
      "code_snippet": "if not (1024 <= bolt_port <= 65535):\n    raise ValueError(f'Invalid NEO4J_BOLT_PORT: {bolt_port}')  # Too permissive",
      "impact": "Potential conflicts with system services, difficult to diagnose port conflicts",
      "fix": "Recommend ephemeral port range (49152-65535) or check against common service ports. Add warning for ports <10000.",
      "references": ["https://www.iana.org/assignments/service-names-port-numbers/"]
    },
    {
      "id": "MED-009",
      "severity": "medium",
      "category": "missing_validation",
      "file": "src/amplihack/memory/neo4j/port_manager.py",
      "line_range": "226-340",
      "title": "No Validation of Port Range Before Auto-Selection",
      "description": "resolve_port_conflicts() uses find_available_port(bolt_port + 100) but doesn't verify +100 doesn't exceed max port (65535). If bolt_port is 65500, this wraps around or fails silently.",
      "code_snippet": "new_bolt = find_available_port(bolt_port + 100)  # Could be > 65535",
      "impact": "Port selection failure at high port ranges, silent failures, confusing errors",
      "fix": "Validate start_port + max_attempts <= 65535. Use modulo arithmetic or better range selection.",
      "references": []
    },
    {
      "id": "MED-010",
      "severity": "medium",
      "category": "error_handling",
      "file": "src/amplihack/memory/neo4j/lifecycle.py",
      "line_range": "157-168",
      "title": "Health Check Doesn't Distinguish Connection vs Query Failure",
      "description": "is_healthy() calls verify_connectivity() but treats all exceptions same way (lines 166-168). Can't distinguish 'Neo4j not started' from 'wrong password' from 'network error'.",
      "code_snippet": "except Exception as e:\n    logger.debug('Health check failed: %s', e)\n    return False  # All errors treated same",
      "impact": "Difficult debugging, poor error messages, can't provide specific fix instructions",
      "fix": "Catch specific exceptions (ServiceUnavailable, AuthError, etc.) and return structured result with error type.",
      "references": []
    },
    {
      "id": "MED-011",
      "severity": "medium",
      "category": "security",
      "file": "src/amplihack/memory/neo4j/auto_setup.py",
      "line_range": "22-36",
      "title": "Password Alphabet Excludes Some Safe Characters",
      "description": "generate_secure_password() uses restricted alphabet (line 35: 'ascii_letters + digits + \"-_=+\"') to avoid Neo4j AUTH string issues. However, this reduces entropy from ~6 bits to ~5.9 bits per character. Documentation doesn't explain which characters are unsafe.",
      "code_snippet": "alphabet = string.ascii_letters + string.digits + '-_=+'  # Why these specific chars?",
      "impact": "Slightly reduced password entropy, unclear security tradeoffs, might be overly conservative",
      "fix": "Document which characters break Neo4j AUTH and why. Test if more punctuation is actually safe. Consider Base64 encoding instead of character restriction.",
      "references": []
    },
    {
      "id": "MED-012",
      "severity": "medium",
      "category": "race_condition",
      "file": "src/amplihack/memory/neo4j/auto_setup.py",
      "line_range": "56-104",
      "title": "TOCTOU in Environment File Creation",
      "description": "auto_create_env_file() checks env_file.exists() (line 59) then checks content (lines 60-65), then writes (line 85). Between check and write, another process could modify or delete file.",
      "code_snippet": "if env_file.exists():\n    content = env_file.read_text()  # File could be deleted here\n    if 'NEO4J_PASSWORD=' in content:\n        return True, ...\n# Later:\nenv_file.write_text(env_content)  # Another process might have written first",
      "impact": "Race condition in concurrent startup, potential file corruption or password loss",
      "fix": "Use atomic file operations. Write to temp file then rename. Use file locking.",
      "references": ["https://docs.python.org/3/library/tempfile.html#tempfile.NamedTemporaryFile"]
    }
  ],
  "low_issues": [
    {
      "id": "LOW-001",
      "severity": "low",
      "category": "type_safety",
      "file": "src/amplihack/memory/database.py",
      "line_range": "49-64",
      "title": "Missing Type Annotation for _get_connection Return Context Manager",
      "description": "_get_connection() returns sqlite3.Connection but is used with 'with' statement. Type should be ContextManager[sqlite3.Connection] or document context manager protocol.",
      "code_snippet": "def _get_connection(self) -> sqlite3.Connection:\n    # Should be: -> ContextManager[sqlite3.Connection]",
      "impact": "Unclear type semantics, type checker might miss context manager protocol violations",
      "fix": "Add from typing import ContextManager; return type -> ContextManager[sqlite3.Connection] or document usage.",
      "references": []
    },
    {
      "id": "LOW-002",
      "severity": "low",
      "category": "documentation",
      "file": "src/amplihack/memory/database.py",
      "line_range": "66-110",
      "title": "Foreign Key Constraint Not Documented in Schema",
      "description": "_create_tables() creates foreign key constraint (line 84: 'FOREIGN KEY (parent_id) REFERENCES memory_entries(id) ON DELETE SET NULL') but doesn't document cascading behavior or implications for hierarchical memories.",
      "code_snippet": "FOREIGN KEY (parent_id) REFERENCES memory_entries(id) ON DELETE SET NULL  # What happens to children?",
      "impact": "Unclear behavior when parent deleted, potential confusion about orphaned children",
      "fix": "Add docstring explaining foreign key semantics. Document that deleting parent sets children's parent_id to NULL.",
      "references": []
    },
    {
      "id": "LOW-003",
      "severity": "low",
      "category": "code_quality",
      "file": "src/amplihack/memory/manager.py",
      "line_range": "222-260",
      "title": "store_batch Continues on Error Instead of Rolling Back",
      "description": "store_batch() catches exceptions for each memory (lines 256-258) and appends None to list but continues processing. If batch is meant to be atomic, this violates atomicity.",
      "code_snippet": "for memory_data in memories:\n    try:\n        memory_id = self.store(**memory_data)\n    except Exception as e:\n        print(...)  # Continue anyway!",
      "impact": "Partial batch success unclear, difficult to reason about consistency",
      "fix": "Add atomic flag: atomic=False by default. If atomic=True, wrap in transaction and rollback on any error.",
      "references": []
    },
    {
      "id": "LOW-004",
      "severity": "low",
      "category": "type_safety",
      "file": "src/amplihack/memory/neo4j/connector.py",
      "line_range": "217-218",
      "title": "Driver Type Annotation as Any Instead of Specific Type",
      "description": "Neo4jConnector stores driver as Optional[Any] (line 217: 'self._driver: Optional[Any] = None') instead of Optional[neo4j.Driver]. Loses type safety for driver operations.",
      "code_snippet": "self._driver: Optional[Any] = None  # Should be: Optional[neo4j.Driver]",
      "impact": "Lost type checking, IDE autocomplete doesn't work, easier to misuse driver",
      "fix": "Import from neo4j import Driver; type as Optional[Driver]. Handle import error gracefully when neo4j not installed.",
      "references": []
    },
    {
      "id": "LOW-005",
      "severity": "low",
      "category": "missing_documentation",
      "file": "src/amplihack/memory/neo4j/memory_store.py",
      "line_range": "120-136",
      "title": "Metadata JSON Serialization Not Documented",
      "description": "create_memory() serializes metadata to JSON string (line 122: 'metadata_json = json.dumps(metadata)') but doesn't document that Neo4j stores as string, not native JSON. Implications for querying not explained.",
      "code_snippet": "metadata_json = json.dumps(metadata) if metadata else '{}'  # Why JSON string?",
      "impact": "Unclear to users that metadata queries require JSON parsing, potential performance issues",
      "fix": "Document that metadata is JSON string. Explain how to query: m.metadata CONTAINS or use apoc.convert.fromJsonMap().",
      "references": []
    },
    {
      "id": "LOW-006",
      "severity": "low",
      "category": "code_quality",
      "file": "src/amplihack/memory/neo4j/lifecycle.py",
      "line_range": "75-113",
      "title": "Stop Method Uses Compose But Create Uses Docker Run",
      "description": "stop() uses docker-compose (lines 92-99) but _create_container() uses direct docker run (lines 243-272). Inconsistent tooling makes it confusing which method works.",
      "code_snippet": "# stop() uses:\nself.config.compose_cmd.split() + ['-f', str(self.config.compose_file), 'stop']\n# _create_container() uses:\n['docker', 'run', '-d', ...]",
      "impact": "Confusion about which tool is used, potential issues if compose file doesn't match docker run args",
      "fix": "Standardize on one approach. Either use compose for both or docker CLI for both. Document choice.",
      "references": []
    },
    {
      "id": "LOW-007",
      "severity": "low",
      "category": "missing_validation",
      "file": "src/amplihack/memory/neo4j/port_manager.py",
      "line_range": "23-40",
      "title": "is_port_in_use Doesn't Distinguish Bind vs Listen",
      "description": "is_port_in_use() uses socket.connect_ex() which only checks if something is listening. Doesn't check if port is bindable. Port might be in TIME_WAIT state or blocked by firewall.",
      "code_snippet": "result = sock.connect_ex((host, port))\nreturn result == 0  # Only checks listening, not bindable",
      "impact": "False negatives: port appears available but can't be bound. Container creation fails mysteriously.",
      "fix": "Also try socket.bind() to verify port is actually bindable. Handle both cases separately.",
      "references": []
    },
    {
      "id": "LOW-008",
      "severity": "low",
      "category": "error_handling",
      "file": "src/amplihack/memory/neo4j/startup_wizard.py",
      "line_range": "15-35",
      "title": "Log Parsing Uses String Contains for Ready Detection",
      "description": "check_container_logs_for_ready() uses simple string search (line 32: 'return \"Started.\" in logs') which could match false positives if log contains error message mentioning 'Started.'.",
      "code_snippet": "return 'Started.' in logs or 'Remote interface available' in logs  # Fragile",
      "impact": "False positive if error message contains 'Started.', premature ready detection",
      "fix": "Use regex to match specific log patterns. Verify line format matches Neo4j startup message format.",
      "references": []
    }
  ],
  "summary": {
    "total_issues": 40,
    "by_severity": {
      "critical": 8,
      "high": 12,
      "medium": 12,
      "low": 8
    },
    "by_category": {
      "connection_management": 3,
      "data_integrity": 3,
      "transaction_safety": 2,
      "injection_risk": 2,
      "performance": 6,
      "error_handling": 5,
      "race_condition": 2,
      "missing_indexes": 3,
      "security": 2,
      "type_safety": 5,
      "code_reuse": 3,
      "missing_validation": 3,
      "documentation": 1
    },
    "key_recommendations": [
      "URGENT: Fix connection pooling in MemoryDatabase to prevent connection leaks (CRIT-001)",
      "URGENT: Add explicit transaction control with rollback in store_memory and Neo4j operations (CRIT-002, CRIT-006)",
      "URGENT: Fix SQL injection risks in retrieve_memories limit/offset and tag search (CRIT-003, HIGH-005)",
      "HIGH PRIORITY: Add composite indexes for common query patterns (HIGH-001, HIGH-008)",
      "HIGH PRIORITY: Replace print() with proper logging throughout database operations (HIGH-002)",
      "HIGH PRIORITY: Fix N+1 query patterns in list_sessions and retrieve_memories (HIGH-004, CRIT-004, HIGH-010)",
      "IMPORTANT: Add jitter to exponential backoff in Neo4j connector (HIGH-006)",
      "IMPORTANT: Verify AgentType exists before creating Memory to prevent orphaned nodes (HIGH-009)",
      "RECOMMENDED: Extract duplicate retry logic and session info queries into helper methods (MED-005, MED-001)",
      "RECOMMENDED: Use environment files or secrets for Neo4j password instead of command line (HIGH-012)"
    ],
    "database_health_assessment": {
      "sqlite_implementation": "Functional but needs connection pooling, transaction improvements, and index optimization",
      "neo4j_implementation": "Good structure but needs transaction verification, index creation, and referential integrity enforcement",
      "overall_risk": "MEDIUM-HIGH due to connection leaks and injection risks. Production readiness requires critical fixes.",
      "performance_concerns": "Multiple N+1 patterns and missing indexes will cause problems at scale",
      "security_concerns": "Password exposure in docker commands, potential injection vectors in dynamic SQL/Cypher"
    }
  }
}
